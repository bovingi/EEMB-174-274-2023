---
title: 'Day 3: Bayesian Updating'
author: "Stephen R. Proulx"
date: "1/18/2023"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
install.packages("rethinking")
library(rethinking)
```

# Calculating posterior probability distribution using grid approximation

Goals for today: \* Understand how to write down the formula for the
posterior distribution \* Use R to perform grid approximation for a
single random trial or a binomial likelihood \* Perform multiple rounds
of Bayesian updating to understand how previous experiments become part
of the prior probability.

## Recipe for doing Bayesian updating by "grid approximation"

Remember that the basic formula is $$
\mathrm{Posterior}(p | d) = \frac{\mathrm{Likelihood}(d | p) * \mathrm{prior}(p)}{\mathrm{norm(d)}}
$$ The function $\mathrm{norm(d)}$ is a function of the data: This is
because it integrates over all possible values of the parameters. So
while this does not depend on the parameters, it does depend on the
choice of possible parameters.

1.  *norm(d) is hard to figure out sometimes, but you're just adding up
    all of the ways that you could get the data. For the covid example,
    you're adding up all of the probabilities of having a runny nose for
    each possible reason you'd had a runny nose (see notes).*

2.  *Independent of the parameter that you're considering.*
    $$norm(d) = Pr(d)$$

3.  Write your likelihood function: This is the model or "data story"
    and reflects your knowledge and assumptions about the biological
    system.

    -   *You have some data (observations) that you understand to have a
        distribution (poisson, binomial, etc.), There can be things in
        this section that area assumptions (i.e. not that we are
        testing), but they're part of the 'small world' that you're
        building.*

4.  Identify the parameters, these are the part of the likelihood
    function that are unknown. Decide what the range of possible
    parameter values is.

    -   *This can be the hypothesis - does something have an effect?*

    -   *The things that you think will have an effect.*

5.  Create a "grid" of the parameters. You have the ability to make this
    grid regular (i.e. each point is the same distance from its
    neighbor) or scaled some other way. This will really be a list of
    points in the grid, i.e. a table where each row is a set of
    parameters to be considered.

6.  Calculate the likelihood of the data for each value of the
    parameters in the grid and store this value.

    -   *you'll end up with a distribution of posterior probabilities
        associated with all the shared values on the grid (see notes)*

7.  Decide on a prior and define it for each of the points in the grid.

    -   *We have some idea of what values a parameter could have
        (preexisting ideas) based on experience, common knowledge,
        previous observations of the system.*

    -   *Prior is a probability distribution - accumulating data is the
        same as changing your prior.*

8.  Multiply the likelihood and the prior and the store this "raw
    posterior" value *(unnormalized posterior)*

9.  Normalize the raw posterior to get the posterior probability.

    -   *Normalize by adding up the values in the grid and divide by sum
        (should equal 1 because they're probabilities).*

10. Take a break and bask in your success!

## Putting it into practice, globe tossing sequential data acquisition

Here we will do the globe tossing model from chapter 2 by updating our
posterior distribution with one piece of data at a time.

1.  Let's write out our likelihood. It has one parameter, the
    probability of water, $p_w$. We could write $$
    \mathrm{Pr}(\mathrm{water} | p_w) = p_w 
    $$ $$ 
    \mathrm{Pr}(\mathrm{land} | p_w) = 1- p_w
    $$

2.  The only parameter in sight is $p_w$ which can be between 0 and 1.
    We may want to assume that values of exactly 0 or exactly 1 are not
    possible (i.e. our prior for values of 0 or 1 is vanishingly small).

3.  Our grid will be one-dimensional, with values ranging from 0 to 1.
    I'll do this by starting a tibble:

```{r}
posterior_table =tibble(p_w_proposed = seq(from=0, to = 1, by = 0.01) ) 
```

I'm calling this column `p_w_proposed` because I want to emphasize that
this is not the value of $p_w$ used to generate the data.

4.  Now we calculate our likelihood. We're going to set the value of
    `d_now` (for "data now") and pass this value to our likelihood
    function. I'll use the coding of 1 for water and 0 for land. Note
    there are lots of ways to do this, I am doing it right inside the
    mutate function itself. The trick here is to use the fact that
    `(1==1)` returns 1, while `1==0` returns 0.

```{r}
d_now=1

posterior_table <- mutate(posterior_table , likelihood = (d_now==1)*p_w_proposed + (d_now==0)*(1-p_w_proposed) )
```

5.  We'll start with a uniform prior, meaning we believe all values of
    $p_w$ are equally likely. Because we are discretising the prior
    density function, I'm going to go ahead and ensure that the
    probabilities add up to 1 by setting our prior probability to 1
    divided by the number of points in our grid.

Again we add this using mutate, and we use the fact that `n()` gives the
number of elements in our grid.

```{r}
posterior_table <- mutate(posterior_table , prior=1/n())
```

You can check to see that the prior sums to 1:

```{r}
sum(posterior_table$prior)
```

6.  Multiply the likelihood and the prior and the store this "raw
    posterior" value. Again `mutate` does the trick.

```{r}
posterior_table <- mutate(posterior_table , raw_posterior = likelihood * prior)
```

7.  Normalize the raw posterior to get the posterior probability. Check
    to see what it sums to:

```{r}
posterior_table <- mutate(posterior_table , posterior = raw_posterior/sum(raw_posterior))
```

Check to see that we have normalized it successfully:

```{r}
sum(posterior_table$posterior)

```

8.  Before you rest: let's see what it looks like.

```{r}
ggplot(data=posterior_table, aes(x=p_w_proposed, y=posterior))+
  geom_line(color="pink")+
  geom_line(aes(x=p_w_proposed, y=prior) , color="lightblue")
```

### Bayesian updating as new information arises

Hope you had a good break! Now let's see how we can turn the crank over
and over again.

First, use the posterior you just generated as your new prior.

```{r}
posterior_table_next <- mutate(posterior_table , prior = posterior)
```

Decide what piece of data to add:

```{r}
d_now=0
```

Turn the crank on the Bayesian updating.

```{r}
posterior_table_next <- mutate(posterior_table_next , 
                               likelihood= (d_now==1)*p_w_proposed+(d_now==0)*(1-p_w_proposed),
                               raw_posterior = likelihood * prior , 
                               posterior = raw_posterior/sum(raw_posterior))

```

```{r}
ggplot(data=posterior_table_next, aes(x=p_w_proposed, y=posterior))+
  geom_line(color="red")+
  geom_line(aes(x=p_w_proposed, y=prior) , color="blue")
```

-   *Now, since we had 1 land and 1 water, the red curve shows that the
    likelihood of land and water is equal (each is .5)*

## Continue updating with the full sequence given in the book

We want to see what happens when we have a sequence of draws as listed
in the book:

W L W W W L W L W

Do this yourself by cutting and pasting the crank-turning code. Be sure
to include the part where we take the posterior from the last iteration
as the prior for the next.

First, use the posterior you just generated as your new prior.

```{r}
posterior_table_next <- mutate(posterior_table_next , prior = posterior)
```

Decide what piece of data to add:

```{r}
d_now=1
```

Turn the crank on the Bayesian updating.

```{r}
posterior_table_next <- mutate(posterior_table_next , 
                               likelihood= (d_now==1)*p_w_proposed+(d_now==0)*(1-p_w_proposed),
                       raw_posterior = likelihood * prior , 
                       posterior = raw_posterior/sum(raw_posterior))

```

```{r}
ggplot(data=posterior_table_next, aes(x=p_w_proposed, y=posterior))+
  geom_line(color="red")+
  geom_line(aes(x=p_w_proposed, y=prior) , color="blue")
```

-   *Red is posterior, blue is prior. Since we're updating the posterior
    with the previous prior each time, this will change with each run.
    What we end up with is shifted to the right, which will tell us that
    the probability of water is higher -- more likely we'd land on
    water. Really low values are unlikely, and values near 1 are out
    since we did see some land (so it can't be 1, i.e. all water).*

-   *The more information we add, the less each addition will shift the
    distribution.*

-   *We were technically using a 'Bernoulli distribution" like a coin
    flip. When we do multiple independent observations, the 'Binomial
    distribution" will tell us what the probability is of getting each
    result.*

## Try changing the order that you add W's and L's to the list. How does it change the outcome?

## Globe tossing binomial style

We can also do this by grouping the W's and L's together ahead of time.
Because we are assuming that the order of the observations does not
matter we can use a binomial likelihood to describe a pool of
observations.

You will write the code to do it yourself and plot the figure. This
additional useful piece of code to describe the binomial likelihood:
`likelihood = dbinom(numW,size=numT,prob=p_w_proposed)` where you will
need to replace `numT` with the number of times the globe has been
tossed (i.e. number of trials) and `numW` with the number of times it
came up W.

-   *Data has 2 components: how many times I flipped the globe, how many
    times it was water.*
    
```{r}
#posterior_table_binom <- mutate(posterior_table, prior = posterior)
posterior_table_binom <- tibble(p_w_proposed = seq(from=0, to = 1, by = 0.01) ) 
```

Decide what piece of data to add:

```{r}
d_now=1
numT= 9
numW = 5
```

Turn the crank on the Bayesian updating.

```{r}
posterior_table_next <- mutate(posterior_table_binom , 
                               likelihood = dbinom(numW,size=numT,prob=p_w_proposed),
                              # likelihood= (d_now==1)*p_w_proposed+(d_now==0)*(1-p_w_proposed),
                       raw_posterior = likelihood * prior , 
                       posterior = raw_posterior/sum(raw_posterior))

```

```{r}
ggplot(data=posterior_table_next, aes(x=p_w_proposed, y=posterior))+
  geom_line(color="red")+
  geom_line(aes(x=p_w_proposed, y=prior) , color="blue")
```


## Using the same technique to answer other types of Bayesian question

Recall our covid diagnosis question. Assume that the probability of cold
symptoms if you have a cold is 100%, while if you have covid it is 25%.
10% of the population has a cold at any given time, and 1% have covid.
If you have cold symptoms, what is the probability that you have covid?

Here the prior is the probability of covid in the absence of new
data/information, in other words the frequency of covid in the entire
population. The "parameter" we are investigating is the patient's covid
infection state, 0 if they don't have covid and 1 if they do.

```{r}
dat=1 # you do have symptoms
pcovid=0.01
pcold=0.25
psym_cov=0.25
posterior_covid <- tibble(covid = seq(from=0, to=1, by=1), prior=c(1-pcovid,pcovid)) %>% 
  mutate(likelihood = dat*(covid*(pcold*1 + (1-pcold)*psym_cov) + (1-covid)*((1-pcold)*0+pcold*1) ) +
           (1-dat)*(covid *( (1-pcold)*(1-psym_cov) + pcold*0 ) + (1-covid)*((1-pcold)*1+(pcold)*0) ),
         raw_posterior=likelihood*prior,
         posterior=raw_posterior/sum(raw_posterior))%>%
  view()
```

Try this with `dat=0` meaning that the data are that the patient does
not have cold symptoms.
